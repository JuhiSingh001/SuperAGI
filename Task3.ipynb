{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Single GPU Training Loop"
      ],
      "metadata": {
        "id": "4qKtAfrzxlGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcrfRGV_xidr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_single_gpu(model, train_dataset, learning_rate, epochs, device):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in train_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed Data Parallel (DDP):"
      ],
      "metadata": {
        "id": "KzpJr9S8xxNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "def train_ddp(model, train_dataset, learning_rate, epochs, rank, world_size):\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "    model.to(rank)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, sampler=train_sampler)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for batch in train_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(rank), targets.to(rank)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if rank == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    dist.destroy_process_group()\n"
      ],
      "metadata": {
        "id": "lF4l3i40xkov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully Sharded Data Parallel (FSDP)"
      ],
      "metadata": {
        "id": "_mm7KyjVx3DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "\n",
        "def train_fsdp(model, train_dataset, learning_rate, epochs, rank, world_size):\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "    model = FSDP(model).to(rank)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, sampler=train_sampler)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for batch in train_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(rank), targets.to(rank)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if rank == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    dist.destroy_process_group()\n"
      ],
      "metadata": {
        "id": "Sa7B5P9Nx7Ec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}