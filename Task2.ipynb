{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rotatory"
      ],
      "metadata": {
        "id": "LYZ6UHSdvfoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n6QgclA6vi4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd5Az8E6vWiX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat([-x2, x1], dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        t = torch.arange(max_seq_len).type_as(inv_freq)\n",
        "        freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
        "        self.register_buffer('cos', freqs.cos())\n",
        "        self.register_buffer('sin', freqs.sin())\n",
        "\n",
        "    def forward(self, q, k):\n",
        "        return apply_rotary_pos_emb(q, k, self.cos, self.sin)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group query attention"
      ],
      "metadata": {
        "id": "2FWpK0W8vnAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PqJY1gPnvvKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupQueryAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, num_groups):\n",
        "        super(GroupQueryAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.num_groups = num_groups\n",
        "        self.head_dim = embed_size // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.query_projections = nn.ModuleList([\n",
        "            nn.Linear(self.head_dim, self.head_dim) for _ in range(num_groups)\n",
        "        ])\n",
        "        self.key_projection = nn.Linear(embed_size, embed_size)\n",
        "        self.value_projection = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        N, seq_length, _ = x.shape\n",
        "        queries = x.reshape(N, seq_length, self.num_heads, self.head_dim)\n",
        "        keys = self.key_projection(x).reshape(N, seq_length, self.num_heads, self.head_dim)\n",
        "        values = self.value_projection(x).reshape(N, seq_length, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Process each group of queries separately\n",
        "        attention_scores = 0\n",
        "        for i, query_projection in enumerate(self.query_projections):\n",
        "            group_queries = query_projection(queries[:, :, i::self.num_groups])\n",
        "            attention_scores += (group_queries @ keys.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(attention_scores, dim=-1)\n",
        "        out = attention @ values\n",
        "        out = out.reshape(N, seq_length, -1)\n",
        "        return self.fc_out(out)\n"
      ],
      "metadata": {
        "id": "1mMUCqpGvxwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sliding window attention"
      ],
      "metadata": {
        "id": "y3VU4SWsv0ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, window_size):\n",
        "        super(SlidingWindowAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        self.query = nn.Linear(self.head_dim, self.head_dim)\n",
        "        self.key = nn.Linear(self.head_dim, self.head_dim)\n",
        "        self.value = nn.Linear(self.head_dim, self.head_dim)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        N, seq_length, _ = x.shape\n",
        "        qkv = x.reshape(N, seq_length, self.num_heads, self.head_dim)\n",
        "        queries, keys, values = map(lambda layer: layer(qkv), (self.query, self.key, self.value))\n",
        "\n",
        "        # Apply sliding window\n",
        "        attention_scores = torch.zeros((N, self.num_heads, seq_length, self.window_size), device=x.device)\n",
        "        for i in range(seq_length):\n",
        "            start = max(0, i - self.window_size + 1)\n",
        "            end = i + 1\n",
        "            attention_scores[:, :, i, :i-start+1] = (queries[:, i] @ keys[:, start:end].transpose(-2, -1))\n",
        "\n",
        "        if mask is not None:\n",
        "            extended_mask = mask[:, None, :, None]\n",
        "            attention_scores = attention_scores.masked_fill(extended_mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(attention_scores, dim=-1)\n",
        "        out = attention @ values\n",
        "        out = out.reshape(N, seq_length, -1)\n",
        "        return self.fc_out(out)\n"
      ],
      "metadata": {
        "id": "hWwJ0ndWv5om"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}